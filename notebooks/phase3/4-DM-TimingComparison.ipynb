{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Timing Comparison Between Massless GravNet and Edge-based Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# System imports\n",
    "import os\n",
    "import sys\n",
    "import yaml\n",
    "\n",
    "# External imports\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "from torch_geometric.data import Data\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from pytorch_lightning import Trainer\n",
    "from pytorch_lightning.loggers import TensorBoardLogger, WandbLogger\n",
    "import torch.utils.benchmark as benchmark\n",
    "import scipy as sp\n",
    "import wandb\n",
    "\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "sys.path.append(\"../../\")\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "from lightning_modules.jetGNN.submodels.interaction_gnn import InteractionGNN\n",
    "from lightning_modules.jetGNN.submodels.gravnet import GravNet\n",
    "from lightning_modules.jetGNN.submodels.particlenet import ParticleNet\n",
    "from lightning_modules.jetGNN.utils import build_edges"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TOP LINE TAKE-AWAYS**\n",
    "1. PyG radius graph appears to be the fastest option for large batch, small graph datasets\n",
    "2. GravNet is around 3-6x faster than ParticleNet (with 3 graph iterations, 64 hidden channels)\n",
    "3. GravNet takes around 10x less memory than ParticleNet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GravNet Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the config file\n",
    "config_file = \"jet_tag_config_2.yaml\"\n",
    "with open(config_file, \"r\") as f:\n",
    "    config = yaml.load(f, Loader=yaml.FullLoader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GravNet(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading torch files\n",
      "Fri Sep 23 05:38:28 2022\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:44<00:00, 44.79s/it]\n",
      "100%|██████████| 1/1 [00:49<00:00, 49.82s/it]\n",
      "100%|██████████| 1/1 [00:52<00:00, 52.55s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building events\n",
      "Fri Sep 23 05:40:56 2022\n",
      "Testing sample quality\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 90000/90000 [00:09<00:00, 9621.01it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing sample quality\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20000/20000 [00:01<00:00, 10658.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing sample quality\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20000/20000 [00:01<00:00, 10814.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defining figures of merit\n",
      "Fri Sep 23 05:41:11 2022\n"
     ]
    }
   ],
   "source": [
    "model.setup(stage=\"fit\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For quick debugging\n",
    "trainset, valset, testset = model.trainset, model.valset, model.testset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.trainset, model.valset, model.testset = trainset, valset, testset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test FRNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explore Time Benchmarking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get current memory allocation\n",
    "torch.cuda.reset_max_memory_allocated()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5189099311828613 GB\n"
     ]
    }
   ],
   "source": [
    "print(torch.cuda.max_memory_allocated() / 1024 ** 3, \"GB\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.to(device)\n",
    "for batch in model.train_dataloader():\n",
    "    sample = batch.to(device)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataBatch(y=[8000], pE=[390341], px=[390341], py=[390341], pz=[390341], log_pt=[390341], log_E=[390341], delta_pt=[390341], log_delta_pt=[390341], delta_E=[390341], log_delta_E=[390341], delta_R=[390341], delta_eta=[390341], delta_phi=[390341], jet_pt=[8000], jet_pE=[8000], jet_px=[8000], jet_py=[8000], jet_pz=[8000], jet_mass=[8000], jet_eta=[8000], jet_phi=[8000], x=[390341], batch=[390341], ptr=[8001])"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    output = model(sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.265493392944336 GB\n"
     ]
    }
   ],
   "source": [
    "print(torch.cuda.max_memory_allocated() / 1024 ** 3, \"GB\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "t0 = benchmark.Timer(\n",
    "    stmt=\"with torch.no_grad(): output = model(sample)\",\n",
    "    globals={\"model\": model, \"sample\": sample, \"device\": device},\n",
    "    label=\"Initial_Run\",\n",
    "    sub_label=\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "bench0 = t0.timeit(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total time: 20.848515960387886ms, giving 0.026060644950484855ms per graph\n"
     ]
    }
   ],
   "source": [
    "print(f\"Total time: {bench0.mean * 1e3}ms, giving {bench0.mean / config['train_batch'] * 1e3}ms per graph\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total time (with 8 spatial dims): 109.29274569964036ms, giving 0.013661593212455046ms per graph\n"
     ]
    }
   ],
   "source": [
    "print(f\"Total time (with 8 spatial dims): {bench0.mean * 1e3}ms, giving {bench0.mean / config['train_batch'] * 1e3}ms per graph\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total time: 1685.99712559022ms, giving 0.021074964069877754ms per graph\n"
     ]
    }
   ],
   "source": [
    "print(f\"Total time: {bench0.mean * 1e3}ms, giving {bench0.mean / config['train_batch'] * 1e3}ms per graph\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ParticleNet Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the config file\n",
    "config_file = \"particlenet_config_1.yaml\"\n",
    "with open(config_file, \"r\") as f:\n",
    "    config = yaml.load(f, Loader=yaml.FullLoader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ParticleNet(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.trainset, model.valset, model.testset = trainset, valset, testset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get current memory allocation\n",
    "torch.cuda.reset_max_memory_allocated()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.49274444580078125 GB\n"
     ]
    }
   ],
   "source": [
    "print(torch.cuda.max_memory_allocated() / 1024 ** 3, \"GB\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.to(device)\n",
    "for batch in model.train_dataloader():\n",
    "    sample = batch.to(device)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataBatch(y=[8000], pE=[393546], px=[393546], py=[393546], pz=[393546], log_pt=[393546], log_E=[393546], delta_pt=[393546], log_delta_pt=[393546], delta_E=[393546], log_delta_E=[393546], delta_R=[393546], delta_eta=[393546], delta_phi=[393546], jet_pt=[8000], jet_pE=[8000], jet_px=[8000], jet_py=[8000], jet_pz=[8000], jet_mass=[8000], jet_eta=[8000], jet_phi=[8000], x=[393546], batch=[393546], ptr=[8001])"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    output = model(sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.880566120147705 GB\n"
     ]
    }
   ],
   "source": [
    "print(torch.cuda.max_memory_allocated() / 1024 ** 3, \"GB\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "t1 = benchmark.Timer(\n",
    "    stmt=\"with torch.no_grad(): output = model(sample)\",\n",
    "    globals={\"model\": model, \"sample\": sample, \"device\": device},\n",
    "    label=\"Initial_Run\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "bench1 = t1.timeit(1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total time: 58.271132829017006ms, giving 0.07283891603627127ms per batch\n"
     ]
    }
   ],
   "source": [
    "print(f\"Total time: {bench1.mean * 1e3}ms, giving {bench1.mean / config['train_batch'] * 1e3}ms per batch\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FRNN: Archived"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Timing and tests with FRNN were not successful - it is not well-suited to large batches of small point clouds!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import frnn\n",
    "from torch_geometric.nn import radius_graph, knn_graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_embed = torch.stack([sample.px, sample.py, sample.pz]).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric import utils as pyg_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_embed, grid_mask = pyg_utils.to_dense_batch(batch_embed, batch = sample.batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_lengths = sample.ptr[1:] - sample.ptr[:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r_max, k_max = 0.01, 10\n",
    "def frnn_batch(batch_embed, batch, ptr, r_max, k_max, grid=None, remove_self_loops=True):\n",
    "    grid_embed, grid_mask = pyg_utils.to_dense_batch(batch_embed, batch = batch)\n",
    "    N_lengths = ptr[1:] - ptr[:-1]\n",
    "    \n",
    "    dists, idxs, nn, grid = get_neighbors(grid_embed, N_lengths, r_max, k_max, grid=grid)\n",
    "\n",
    "    idxs = (idxs[grid_mask] + sample.ptr[sample.batch].unsqueeze(1))\n",
    "    positive_idxs = (idxs.squeeze() >= 0)\n",
    "    ind = torch.arange(idxs.shape[0], device = positive_idxs.device).unsqueeze(1).expand(idxs.shape)\n",
    "    edges = torch.stack([ind[positive_idxs],\n",
    "                        idxs[positive_idxs]\n",
    "                        ], dim = 0)  \n",
    "    if remove_self_loops:\n",
    "        edges = edges[:, edges[0] != edges[1]]\n",
    "    return edges, grid\n",
    "\n",
    "\n",
    "def get_neighbors(grid_embed, N_lengths, r_max, k_max, grid=None):\n",
    "    dists, idxs, nn, grid = frnn.frnn_grid_points(points1 = grid_embed,\n",
    "                                          points2 = grid_embed,\n",
    "                                          lengths1 = N_lengths,\n",
    "                                          lengths2 = N_lengths,\n",
    "                                          K = k_max,\n",
    "                                          r = r_max,\n",
    "                                            grid = grid\n",
    "                                         )\n",
    "    return dists, idxs, nn, grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frnn_edges, grid = frnn_batch(batch_embed, sample.batch, sample.ptr, r_max, k_max)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frnn_edges, _ = frnn_batch(batch_embed, sample.batch, sample.ptr, r_max, k_max, grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pyg_edges = radius_graph(batch_embed, r=r_max, max_num_neighbors=k_max, batch=sample.batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frnn_whole_function = benchmark.Timer(\n",
    "    stmt=\"frnn_batch(batch_embed, sample.batch, sample.ptr, r_max, k_max, grid)\",\n",
    "    globals=globals(),\n",
    "    label=\"frnn_batch\",\n",
    ")\n",
    "\n",
    "frnn_neighbors = benchmark.Timer(\n",
    "    stmt=\"get_neighbors(grid_embed, N_lengths, r_max, k_max, grid)\",\n",
    "    globals=globals(),\n",
    "    label=\"get_neighbors\",\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frnn_bench = frnn_whole_function.timeit(number=10)\n",
    "frnn_neighbors_bench = frnn_neighbors.timeit(number=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pyg_t = benchmark.Timer(\n",
    "    stmt=\"radius_graph(batch_embed, r=r_max, max_num_neighbors=16, batch=sample.batch)\",\n",
    "    globals=globals(),\n",
    "    label=\"pyg_radius_graph\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pyg_knn = benchmark.Timer(\n",
    "    stmt=\"knn_graph(batch_embed, 16, batch=sample.batch)\",\n",
    "    globals=globals(),\n",
    "    label=\"pyg_knn\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pyg_bench = pyg_t.timeit(1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pygknn_bench = pyg_knn.timeit(1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PyG radius graph is *1000x* faster than the FRNN implementation (and ~2x faster than PyG knn graph)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.12 ('exatrkx-cori': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "70f08f231fd793b6e9065bb0a869f03e856cf92214029e7ffd4730124f198a39"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
